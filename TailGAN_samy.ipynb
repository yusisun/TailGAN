{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_epochs': 3000, 'batch_size': 1000, 'lr_D': 1e-07, 'lr_G': 1e-06, 'temp': 0.01, 'b1': 0.5, 'b2': 0.999, 'latent_dim': 1000, 'len': 500, 'n_rows': 5, 'n_cols': 100, 'n_critic_G': 1, 'n_critic_D': 1, 'static_way': 'LShort', 'strategies': ['Port', 'MR', 'TF'], 'n_trans': 50, 'Cap': 10, 'WH': 10, 'ratios': [1.0, 1.0], 'thresholds_pct': [[31, 69]], 'data_name': 'test_data_samy_intraday.csv', 'tickers': ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'NVDA'], 'noise_name': 't5', 'alphas': [0.05], 'W': 10.0, 'score': 'quant', 'numNN': 5, 'project': True, 'version': 'Single'}\n",
      "------ Model 0 Starts with Random Seed 887 \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data01/Chao_TailGAN/Static_Port_Transform/LShort_Stk600/TransMat_IS.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 545\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    544\u001b[0m     Write_Opt(opt)\n\u001b[0;32m--> 545\u001b[0m     \u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 506\u001b[0m, in \u001b[0;36mTrain\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m    504\u001b[0m seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------ Model \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m Starts with Random Seed \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (iii, seed))\n\u001b[0;32m--> 506\u001b[0m loss_dge \u001b[38;5;241m=\u001b[39m \u001b[43mTrain_Single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miii\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loss_dge[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.04\u001b[39m:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m * \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 394\u001b[0m, in \u001b[0;36mTrain_Single\u001b[0;34m(opt, dataloader, model_index, seed)\u001b[0m\n\u001b[1;32m    391\u001b[0m optimizer_D\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# Adversarial loss\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m PNL, PNL_validity \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_R\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m gen_PNL, gen_PNL_validity \u001b[38;5;241m=\u001b[39m discriminator(gen_R)\n\u001b[1;32m    396\u001b[0m real_score \u001b[38;5;241m=\u001b[39m criterion(PNL_validity, PNL)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 256\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, R)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, R):\n\u001b[0;32m--> 256\u001b[0m     PNL \u001b[38;5;241m=\u001b[39m \u001b[43mCompute_PNL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     PNL_transpose \u001b[38;5;241m=\u001b[39m PNL\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    258\u001b[0m     PNL_s \u001b[38;5;241m=\u001b[39m PNL_transpose\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mPNL_transpose\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 168\u001b[0m, in \u001b[0;36mCompute_PNL\u001b[0;34m(R)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mCompute_PNL\u001b[39m(R):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# convert returns to PnLs\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     prices_l \u001b[38;5;241m=\u001b[39m Inc2Price(R)\n\u001b[0;32m--> 168\u001b[0m     port_prices_l \u001b[38;5;241m=\u001b[39m \u001b[43mStaticPort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprices_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_trans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_way\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     PNL_BH \u001b[38;5;241m=\u001b[39m BuyHold(prices_l, opt\u001b[38;5;241m.\u001b[39mCap)\n\u001b[1;32m    171\u001b[0m     PNL_l \u001b[38;5;241m=\u001b[39m [PNL_BH]\n",
      "File \u001b[0;32m~/Documents/Berkeley/Synthera.ia/Tail-GAN-main/Transform.py:53\u001b[0m, in \u001b[0;36mStaticPort\u001b[0;34m(prices_l, n_trans, static_way, insample)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     store_path \u001b[38;5;241m=\u001b[39m join(trans_data_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransMat_OOS.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m trans_mat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m trans_mat \u001b[38;5;241m=\u001b[39m Tensor(trans_mat[:, :n_trans])\n\u001b[1;32m     56\u001b[0m swap_prices \u001b[38;5;241m=\u001b[39m prices_l\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data01/Chao_TailGAN/Static_Port_Transform/LShort_Stk600/TransMat_IS.npy'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tail-GAN for synthetic data\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from os.path import join\n",
    "import time\n",
    "import torch\n",
    "#import dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "from Transform import *\n",
    "from gen_thresholds import gen_thresholds\n",
    "from util import *\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "#Initial code\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--n_epochs\", type=int, default=3000, help=\"epochs for training\")\n",
    "# parser.add_argument(\"--batch_size\", type=int, default=1000, help=\"size of the batches\")\n",
    "# parser.add_argument(\"--lr_D\", type=float, default=1e-7, help=\"learning rate for Discriminator\")\n",
    "# parser.add_argument(\"--lr_G\", type=float, default=1e-6, help=\"learning rate for Generator\")\n",
    "# parser.add_argument('--temp', type=float, default=0.01, help='multiplier of temperature')\n",
    "# parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "# parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of second order momentum of gradient\")\n",
    "# parser.add_argument(\"--latent_dim\", type=int, default=1000, help=\"dimensionality of the latent space\")\n",
    "# parser.add_argument(\"--len\", type=int, default=50000, help=\"number of examples\")\n",
    "# parser.add_argument(\"--n_rows\", type=int, default=5, help=\"number of rows\")\n",
    "# parser.add_argument(\"--n_cols\", type=int, default=100, help=\"number of columns\")\n",
    "# parser.add_argument(\"--n_critic_G\", type=int, default=1, help=\"number of training steps for discriminator per iter\")\n",
    "# parser.add_argument(\"--n_critic_D\", type=int, default=1, help=\"number of training steps for generator per iter\")\n",
    "# parser.add_argument(\"--static_way\", type=str, default='LShort', help=\"trading way of static portfolios\")\n",
    "# parser.add_argument(\"--strategies\", type=list, default=['Port', 'MR', 'TF'], help=\"a list of strategy names\")\n",
    "# parser.add_argument(\"--n_trans\", type=int, default=50, help=\"number of static portfolios\")\n",
    "# parser.add_argument(\"--Cap\", type=int, default=10, help=\"maximum investment capital\")\n",
    "# parser.add_argument(\"--WH\", type=int, default=10, help=\"window history for strategy\")\n",
    "# parser.add_argument(\"--ratios\", type=list, default=[1.0, 1.0], help=\"ratios for longing or shorting\")\n",
    "# parser.add_argument(\"--thresholds_pct\", type=list, default=[[31, 69]], help=\"thresholds for longing or shorting\")\n",
    "# parser.add_argument(\"--data_name\", type=str, default='1_Gauss+1_AR50+1_AR-12+1_GARCH-T5+1_GARCH-T10', help=\"data name\")\n",
    "# parser.add_argument(\"--tickers\", type=list, default=['Gauss', 'AR50', 'AR-12', 'GARCH-T5', 'GARCH-T10'], help=\"tickers\")\n",
    "# parser.add_argument(\"--noise_name\", type=str, default='t5', help=\"noise name\")\n",
    "# parser.add_argument(\"--alphas\", type=list, default=[0.05], help=\"quantiles\")\n",
    "# parser.add_argument(\"--W\", type=float, default=10.0, help=\"scale parameter for W\")\n",
    "# parser.add_argument(\"--score\", type=str, default='quant', help=\"score function\")\n",
    "# parser.add_argument(\"--numNN\", type=int, default=5, help=\"number of NNs\")\n",
    "# parser.add_argument(\"--project\", type=bool, default=True, help=\"Project into constraint set\")\n",
    "# parser.add_argument(\"--version\", type=str, default='Single', help=\"version number\")\n",
    "\n",
    "\n",
    "# opt = parser.parse_args()\n",
    "# print(opt)\n",
    "# R_shape = (opt.n_rows, opt.n_cols)\n",
    "\n",
    "\n",
    "\n",
    "#adaptation to a notebook\n",
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.n_epochs = 3000\n",
    "        self.batch_size = 1000\n",
    "        self.lr_D = 1e-7\n",
    "        self.lr_G = 1e-6\n",
    "        self.temp = 0.01\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.latent_dim = 1000\n",
    "        #self.len = 50000\n",
    "        self.len = 500\n",
    "        self.n_rows = 5\n",
    "        self.n_cols = 100\n",
    "        self.n_critic_G = 1\n",
    "        self.n_critic_D = 1\n",
    "        self.static_way = 'LShort'\n",
    "        self.strategies = ['Port', 'MR', 'TF']\n",
    "        self.n_trans = 50\n",
    "        self.Cap = 10\n",
    "        self.WH = 10\n",
    "        self.ratios = [1.0, 1.0]\n",
    "        self.thresholds_pct = [[31, 69]]\n",
    "        #self.data_name = '1_Gauss+1_AR50+1_AR-12+1_GARCH-T5+1_GARCH-T10'\n",
    "        #self.tickers = ['Gauss', 'AR50', 'AR-12', 'GARCH-T5', 'GARCH-T10']\n",
    "        self.data_name = 'test_data_samy_intraday.csv'\n",
    "        self.tickers = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'NVDA']\n",
    "        self.noise_name = 't5'\n",
    "        self.alphas = [0.05]\n",
    "        self.W = 10.0\n",
    "        self.score = 'quant'\n",
    "        self.numNN = 5\n",
    "        self.project = True\n",
    "        self.version = 'Single'\n",
    "\n",
    "\n",
    "opt = Options()\n",
    "print(opt.__dict__)\n",
    "R_shape = (opt.n_rows, opt.n_cols)\n",
    "\n",
    "\n",
    "def Infer_Shape(R_shape):\n",
    "    \"\"\"\n",
    "    Shape of portfolio returns\n",
    "    :param R_shape: shape of asset returns\n",
    "    \"\"\"\n",
    "    PNL_shape_0 = R_shape[0]\n",
    "    for strategy in opt.strategies:\n",
    "        if strategy == 'Port':\n",
    "            PNL_shape_0 += opt.n_trans\n",
    "        elif strategy == 'MR':\n",
    "            PNL_shape_0 += opt.n_rows * len(opt.thresholds_pct)\n",
    "        elif strategy == 'TF':\n",
    "            PNL_shape_0 += opt.n_rows * len(opt.thresholds_pct)\n",
    "        else:\n",
    "            pass\n",
    "    PNL_shape = (PNL_shape_0, R_shape[1])\n",
    "    return PNL_shape\n",
    "\n",
    "PNL_shape = Infer_Shape(R_shape)\n",
    "\n",
    "# Specific version\n",
    "this_version = '_'.join(\n",
    "    [opt.version,\n",
    "     'Stk' + str(opt.n_rows),\n",
    "     opt.noise_name,\n",
    "     'E' + str(opt.n_epochs),\n",
    "     'BS' + str(opt.batch_size),\n",
    "     opt.static_way,\n",
    "     '_'.join(opt.strategies),\n",
    "     'P' + str(opt.n_trans),\n",
    "     'Cap' + str(opt.Cap),\n",
    "     'WH' + str(opt.WH),\n",
    "     'R' + '+'.join([str(a) for a in opt.ratios]),\n",
    "     'T' + '+'.join(['_'.join(map(str, i)) for i in opt.thresholds_pct]),\n",
    "     'D' + str(opt.n_critic_D), 'G' + str(opt.n_critic_G),\n",
    "     'LR' + '-'.join([str(opt.lr_D), str(opt.lr_G)]),\n",
    "     'Temp' + str(opt.temp),\n",
    "     'Q' + '+'.join([str(int(100 * a)) for a in opt.alphas]),\n",
    "     'Esb' + str(opt.numNN)])\n",
    "\n",
    "\n",
    "# Generated Data Storage Path\n",
    "gen_data_path = \"/Users/samykobbite/Documents/Berkeley/Synthera.ia/Tail-GAN-main/gen_data_%s/\" % this_version\n",
    "os.makedirs(gen_data_path, exist_ok=True)\n",
    "\n",
    "# Model Storage Path\n",
    "model_path = \"/Users/samykobbite/Documents/Berkeley/Synthera.ia/Tail-GAN-main/model_%s/\" % this_version\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "\n",
    "def Compute_PNL(R):\n",
    "    # convert returns to PnLs\n",
    "    prices_l = Inc2Price(R)\n",
    "    port_prices_l = StaticPort(prices_l, opt.n_trans, opt.static_way, insample=True)\n",
    "\n",
    "    PNL_BH = BuyHold(prices_l, opt.Cap)\n",
    "    PNL_l = [PNL_BH]\n",
    "\n",
    "    for strategy in opt.strategies:\n",
    "        if strategy == 'Port':\n",
    "            PNL_BHPort = BuyHold(port_prices_l, opt.Cap)\n",
    "            PNL_l.append(PNL_BHPort)\n",
    "        elif strategy == 'MR':\n",
    "            for percentile_l in opt.thresholds_pct:\n",
    "                thresholds_array = gen_thresholds(opt.data_name, opt.tickers, strategy, percentile_l, 100, opt.WH)\n",
    "                PNL_MR = MeanRev(prices_l, opt.Cap, opt.WH, LR=opt.ratios[0], SR=opt.ratios[1],\n",
    "                                 ST=thresholds_array[:, -1], LT=thresholds_array[:, -2])\n",
    "                PNL_l.append(PNL_MR)\n",
    "        elif strategy == 'TF':\n",
    "            for percentile_l in opt.thresholds_pct:\n",
    "                thresholds_array = gen_thresholds(opt.data_name, opt.tickers, strategy, percentile_l, 100, opt.WH)\n",
    "                PNL_TF = TrendFollow(prices_l, opt.Cap, opt.WH, LR=opt.ratios[0], SR=opt.ratios[1],\n",
    "                                     ST=thresholds_array[:, 0], LT=thresholds_array[:, 1])\n",
    "                PNL_l.append(PNL_TF)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    PNL = torch.cat(PNL_l, dim=1)\n",
    "    return PNL\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(R_shape))),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = torch.clamp(img, min=-1, max=1)\n",
    "        img = img.view(img.shape[0], *R_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "temperature_annealing_func = lambda step: opt.temp/torch.log(Tensor([2.7183 + step]))\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.W = opt.W\n",
    "\n",
    "        self.project = opt.project\n",
    "\n",
    "        self.alphas = opt.alphas\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(opt.batch_size, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 2 * len(opt.alphas)),\n",
    "        )\n",
    "\n",
    "    def project_op(self, validity):\n",
    "        for i, alpha in enumerate(self.alphas):\n",
    "            v = validity[:, 2*i].clone()\n",
    "            e = validity[:, 2*i+1].clone()\n",
    "            indicator = torch.sign(torch.as_tensor(0.5 - alpha))\n",
    "            validity[:, 2*i] = indicator * ((self.W * v < e).float() * v + (self.W * v >= e).float() * (v + self.W * e) / (1 + self.W ** 2))\n",
    "            validity[:, 2*i+1] = indicator * ((self.W * v < e).float() * e + (self.W * v >= e).float() * self.W * (v + self.W * e) / (1 + self.W ** 2))\n",
    "            # validity[:, 2*i] = indicator * ((self.W * v < e).float() * (e < v).float() * v + (self.W * v >= e).float() * (v + self.W * e) / (1 + self.W ** 2) + (e >= v).float() * (v + e) / 2.0)\n",
    "            # validity[:, 2*i+1] = indicator * ((self.W * v < e).float() * (e < v).float() * e + (self.W * v >= e).float() * self.W * (v + self.W * e) / (1 + self.W ** 2) + (e >= v).float() * (v + e) / 2.0)\n",
    "        return validity\n",
    "\n",
    "\n",
    "    def forward(self, R):\n",
    "        PNL = Compute_PNL(R)\n",
    "        PNL_transpose = PNL.T\n",
    "        PNL_s = PNL_transpose.reshape(*PNL_transpose.shape, 1)\n",
    "        perm_matrix = deterministic_NeuralSort(PNL_s, opt.temp)\n",
    "        PNL_sort = torch.bmm(perm_matrix, PNL_s)\n",
    "        PNL_validity = self.model(PNL_sort.reshape(*PNL_transpose.shape))\n",
    "        if self.project:\n",
    "            PNL_validity = self.project_op(PNL_validity)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        return PNL, PNL_validity\n",
    "\n",
    "\n",
    "def G1(v):\n",
    "    return v\n",
    "\n",
    "\n",
    "def G2(e, scale=1):\n",
    "    return scale * torch.exp(e / scale)\n",
    "\n",
    "\n",
    "def G2in(e, scale=1):\n",
    "    return scale ** 2 * torch.exp(e / scale)\n",
    "\n",
    "\n",
    "def G1_quant(v, W=opt.W):\n",
    "    return - W * v ** 2 / 2\n",
    "\n",
    "\n",
    "def G2_quant(e, alpha):\n",
    "    return alpha * e\n",
    "\n",
    "\n",
    "def G2in_quant(e, alpha):\n",
    "    return alpha * e ** 2 / 2\n",
    "\n",
    "\n",
    "def S_stats(v, e, X, alpha):\n",
    "    \"\"\"\n",
    "    For a given quantile, here named alpha, calculate the score function value\n",
    "    \"\"\"\n",
    "    if alpha < 0.5:\n",
    "        rt = ((X<=v).float() - alpha) * (G1(v) - G1(X)) + 1. / alpha * G2(e) * (X<=v).float() * (v - X) + G2(e) * (e - v) - G2in(e)\n",
    "    else:\n",
    "        alpha_inverse = 1 - alpha\n",
    "        rt = ((X>=v).float() - alpha_inverse) * (G1(X) - G1(v)) + 1. / alpha_inverse * G2(-e) * (X>=v).float() * (X - v) + G2(-e) * (v - e) - G2in(-e)\n",
    "    return torch.mean(rt)\n",
    "\n",
    "\n",
    "def S_quant(v, e, X, alpha, W=opt.W):\n",
    "    \"\"\"\n",
    "    For a given quantile, here named alpha, calculate the score function value\n",
    "    \"\"\"\n",
    "    if alpha < 0.5:\n",
    "        rt = ((X<=v).float() - alpha) * (G1_quant(v,W) - G1_quant(X,W)) + 1. / alpha * G2_quant(e,alpha) * (X<=v).float() * (v - X) + G2_quant(e,alpha) * (e - v) - G2in_quant(e,alpha)\n",
    "    else:\n",
    "        alpha_inverse = 1 - alpha\n",
    "        rt = ((X>=v).float() - alpha_inverse) * (G1_quant(v,W) - G1_quant(X,W)) + 1. / alpha_inverse * G2_quant(-e,alpha_inverse) * (X>=v).float() * (X - v) + G2_quant(-e,alpha_inverse) * (v - e) - G2in_quant(-e,alpha_inverse)\n",
    "    return torch.mean(rt)\n",
    "\n",
    "\n",
    "class Score(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Score, self).__init__()\n",
    "        self.alphas = opt.alphas\n",
    "        self.score_name = opt.score\n",
    "        if self.score_name == 'quant':\n",
    "            self.score_alpha = S_quant\n",
    "        elif self.score_name == 'stats':\n",
    "            self.score_alpha = S_stats\n",
    "        else:\n",
    "            self.score_alpha = None\n",
    "\n",
    "    def forward(self, PNL_validity, PNL):\n",
    "        # Score\n",
    "        loss = 0\n",
    "        for i, alpha in enumerate(self.alphas):\n",
    "            PNL_var = PNL_validity[:, [2 * i]]\n",
    "            PNL_es = PNL_validity[:, [2 * i + 1]]\n",
    "            loss += self.score_alpha(PNL_var, PNL_es, PNL.T, alpha)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "def Train_Single(opt, dataloader, model_index, seed):\n",
    "    start_time = time.time()\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Initialize generator and discriminator\n",
    "    generator = Generator()\n",
    "    discriminator = Discriminator()\n",
    "    criterion = Score()\n",
    "\n",
    "    if cuda:\n",
    "        generator.cuda()\n",
    "        discriminator.cuda()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr_G, betas=(opt.b1, opt.b2))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr_D, betas=(opt.b1, opt.b2))\n",
    "\n",
    "    loss_d_l = []\n",
    "    loss_g_l = []\n",
    "\n",
    "    gen_size = 1000 # opt.len\n",
    "    for epoch in range(opt.n_epochs):\n",
    "        epoch_loss_D = []\n",
    "        epoch_loss_G = []\n",
    "\n",
    "        for i, R in enumerate(dataloader):\n",
    "\n",
    "            # Configure input\n",
    "            real_R = Variable(R.type(Tensor))\n",
    "\n",
    "            # Sample noise as generator input, (batchsize, latent dim): (1000, 100)\n",
    "            if 't' in opt.noise_name:\n",
    "                z = Variable(\n",
    "                    Tensor(np.random.standard_t(int(opt.noise_name.split('t')[1]), (R.shape[0], opt.latent_dim))))\n",
    "            else:\n",
    "                z = Variable(Tensor(np.random.normal(0, 1, (R.shape[0], opt.latent_dim))))\n",
    "\n",
    "            # Generate a batch of images, (1000, 2, 100)\n",
    "            gen_R = generator(z)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            # Train the generator every n_critic iterations\n",
    "            if i % opt.n_critic_D == 0:\n",
    "\n",
    "                optimizer_D.zero_grad()\n",
    "\n",
    "                # Adversarial loss\n",
    "                PNL, PNL_validity = discriminator(real_R)\n",
    "                gen_PNL, gen_PNL_validity = discriminator(gen_R)\n",
    "                real_score = criterion(PNL_validity, PNL)\n",
    "                fake_score = criterion(gen_PNL_validity, PNL)\n",
    "                loss_D = real_score - fake_score\n",
    "\n",
    "                # Update the Gradient in Discriminator\n",
    "                loss_D.backward(retain_graph=True)\n",
    "                optimizer_D.step()\n",
    "\n",
    "                epoch_loss_D.append(loss_D.item())\n",
    "\n",
    "            # Train the generator every n_critic iterations\n",
    "            if i % opt.n_critic_G == 0:\n",
    "\n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "\n",
    "                optimizer_G.zero_grad()\n",
    "\n",
    "                # Adversarial loss\n",
    "                gen_PNL, gen_PNL_validity = discriminator(gen_R)\n",
    "                loss_G = criterion(gen_PNL_validity, PNL)\n",
    "\n",
    "                # Update the Gradient in Generator\n",
    "                loss_G.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "                epoch_loss_G.append(loss_G.item())\n",
    "\n",
    "        D_loss_epoch = np.mean(epoch_loss_D)\n",
    "        G_loss_epoch = np.mean(epoch_loss_G)\n",
    "\n",
    "        loss_d_l.append(D_loss_epoch)\n",
    "        loss_g_l.append(G_loss_epoch)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"[Epoch %d] [D loss: %.4f] [G loss: %.4f]\" % (epoch, D_loss_epoch, G_loss_epoch))\n",
    "            print(\"--- %d seconds passed ---\" % (time.time() - start_time))\n",
    "\n",
    "        if 't' in opt.noise_name:\n",
    "            z = Variable(\n",
    "                Tensor(np.random.standard_t(int(opt.noise_name.split('t')[1]), (gen_size, opt.latent_dim))))\n",
    "        else:\n",
    "            z = Variable(Tensor(np.random.normal(0, 1, (gen_size, opt.latent_dim))))\n",
    "\n",
    "        gen_R = generator(z)\n",
    "\n",
    "        np.save(join(gen_data_path, \"Fake_id%d_E%d.npy\" % (model_index, epoch)), gen_R.cpu().detach().numpy())\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            # Save the Intermediate Model\n",
    "            discriminator_path = join(model_path, \"discriminator_id%d_E%d\" % (model_index, epoch))\n",
    "            generator_path = join(model_path, \"generator_id%d_E%d\" % (model_index, epoch))\n",
    "            torch.save(discriminator.state_dict(), discriminator_path)\n",
    "            torch.save(generator.state_dict(), generator_path)\n",
    "\n",
    "    # Save Loss Value\n",
    "    loss_d_l = np.array(loss_d_l)\n",
    "    loss_g_l = np.array(loss_g_l)\n",
    "    loss_dge = np.stack([loss_d_l, loss_g_l])\n",
    "    np.save(join(gen_data_path, 'loss_id%d.npy' % model_index), loss_dge)\n",
    "    return loss_dge\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Dataset_IS(Dataset):\n",
    "    def __init__(self, tickers, data_path, length):\n",
    "        self.tickers = tickers\n",
    "        self.data_path = data_path\n",
    "        self.length = length\n",
    "        self.data = self.load_data()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        sample = self.data[idx]\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def load_data(self):\n",
    "        df = pd.read_csv(self.data_path)\n",
    "        return df[self.tickers].values.T\n",
    "    \n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def Train(opt):\n",
    "    \"\"\"\n",
    "    Train multiple Tail-GANs with different random seedsÃ¥\n",
    "    \"\"\"\n",
    "    # Configure data loader\n",
    "    #dataset = Dataset_IS(tickers=opt.tickers, data_path=\"/data01/Chao_TailGAN/gan_data/%s\" % opt.data_name, length=opt.len)\n",
    "    dataset = Dataset_IS(tickers=opt.tickers, data_path=\"/Users/samykobbite/Documents/Berkeley/Synthera.ia/Tail-GAN-main/gan_data/%s\" % opt.data_name, length=opt.len)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=opt.batch_size,\n",
    "                                             shuffle=True)\n",
    "\n",
    "    for iii in range(opt.numNN):\n",
    "        seed = np.random.randint(low=1, high=10000)\n",
    "        print(\"------ Model %d Starts with Random Seed %d \" % (iii, seed))\n",
    "        loss_dge = Train_Single(opt, dataloader, model_index=iii, seed=seed)\n",
    "\n",
    "        while loss_dge[-10:, 1].mean() > 0.04:\n",
    "            print(' * ' * 20)\n",
    "            print('  Attention!!!   Restart Training!!!  ')\n",
    "            print(' * ' * 20)\n",
    "            seed = np.random.randint(low=1, high=10000)\n",
    "            loss_dge = Train_Single(opt, dataloader, model_index=iii, seed=seed)\n",
    "\n",
    "\n",
    "# def Write_Opt(opt):\n",
    "#     \"\"\"\n",
    "#     Save the arguements for each experiment\n",
    "#     \"\"\"\n",
    "#     opt_dic = vars(opt)\n",
    "#     res = \", \".join((\"{}={}\".format(*i) for i in opt_dic.items()))\n",
    "#     argument_path = \"/data01/Chao_TailGAN/Arguments/\"\n",
    "#     os.makedirs(argument_path, exist_ok=True)\n",
    "#     save_path = join(argument_path, this_version + \".txt\")\n",
    "#     text_file = open(save_path, \"wt\")\n",
    "#     n = text_file.write(res)\n",
    "#     text_file.close()\n",
    "\n",
    "\n",
    "def Write_Opt(opt):\n",
    "    \"\"\"\n",
    "    Save the arguments for each experiment\n",
    "    \"\"\"\n",
    "    opt_dic = vars(opt)\n",
    "    res = \", \".join((\"{}={}\".format(*i) for i in opt_dic.items()))\n",
    "    argument_path = \"/Users/samykobbite/Documents/Berkeley/Synthera.ia/Tail-GAN-main/Arguments/\"\n",
    "    os.makedirs(argument_path, exist_ok=True)\n",
    "    save_path = join(argument_path, this_version + \".txt\")\n",
    "    with open(save_path, \"wt\") as text_file:\n",
    "        text_file.write(res)\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Write_Opt(opt)\n",
    "    Train(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
